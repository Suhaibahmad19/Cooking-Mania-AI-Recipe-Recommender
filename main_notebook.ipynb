{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfnWRJ17sC3m",
        "outputId": "de74feef-36de-44e7-c74e-61bb35fa2846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import ast\n",
        "import math\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ct7SMxsiWMO"
      },
      "outputs": [],
      "source": [
        "def preprocess_user_input(user_input):\n",
        "\n",
        "    words = user_input.split()\n",
        "    lemmatizedWords = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "\n",
        "        lemword = lemmatizer.lemmatize(word)\n",
        "\n",
        "        lemmatizedWords.append(lemword)\n",
        "\n",
        "    final_output = ' '.join(lemmatizedWords)\n",
        "\n",
        "    return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOyP92leHJy0"
      },
      "outputs": [],
      "source": [
        "# Cosine similarity function\n",
        "def cosine_similarity(v1, v2):\n",
        "\n",
        "    dotProduct = 0\n",
        "    for x,y in zip(v1,v2):\n",
        "        dotProduct += x*y\n",
        "\n",
        "    normv1 = 0\n",
        "    for x in v1:\n",
        "        normv1 += x ** 2\n",
        "    normv1 = normv1 ** 0.5\n",
        "\n",
        "    normv2 = 0\n",
        "    for x in v2:\n",
        "        normv2 += x ** 2\n",
        "    normv2 = normv2 ** 0.5\n",
        "\n",
        "    if normv1 == 0 or normv2 == 0:\n",
        "        return 0.0\n",
        "    similarity = dotProduct / (normv1 * normv2)\n",
        "\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-fRFMxfDQUJ"
      },
      "outputs": [],
      "source": [
        "def parse_ingredients(ingredients_list):\n",
        "    # Remove any extra characters such as quotes or spaces around the list\n",
        "    cleaned_ingredients = ingredients_list.strip().strip(\"'\").strip(\"[]\")\n",
        "    # Split by commas and remove any extra spaces\n",
        "    ingredients = cleaned_ingredients.split(\"', '\")\n",
        "    final = \" \".join(ingredients)\n",
        "    print(final)\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFrHSEneHFXy"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(filepath):\n",
        "    # Read file manually without pandas\n",
        "    with open(filepath, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    header = lines[0].strip().split(',')\n",
        "    data = []\n",
        "\n",
        "    for line in lines[1:]:\n",
        "        row = line.strip().split(',')\n",
        "        # Handle cases where the row has fewer elements than the header\n",
        "        row_data = {header[i]: row[i] if i < len(row) else '' for i in range(len(header))}\n",
        "        data.append(row_data)\n",
        "\n",
        "    # Extract required columns and process ingredients\n",
        "    for row in data:\n",
        "        # Clean and parse the ingredients list\n",
        "        row['ingredients_text'] = parse_ingredients(row['ingredients_list'])\n",
        "\n",
        "    # Create a vocabulary for TF-IDF and compute TF-IDF values manually\n",
        "    vocabulary = {}\n",
        "    doc_count = {}\n",
        "    tfidf_matrix = []\n",
        "\n",
        "    # Build vocabulary and term frequencies\n",
        "    for row in data:\n",
        "        words = row['ingredients_text'].split()\n",
        "        row_tfidf = {}\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = len(vocabulary)\n",
        "                doc_count[word] = 0\n",
        "            doc_count[word] += 1\n",
        "            row_tfidf[word] = row_tfidf.get(word, 0) + 1\n",
        "        tfidf_matrix.append(row_tfidf)\n",
        "\n",
        "    # Compute TF-IDF for each word\n",
        "    total_docs = len(data)\n",
        "    dense_tfidf_matrix = []\n",
        "    for row_tfidf in tfidf_matrix:\n",
        "        dense_row = [0] * len(vocabulary)\n",
        "        for word, count in row_tfidf.items():\n",
        "            tf = count\n",
        "            idf = math.log(total_docs / (1 + doc_count[word]))  # Applying smoothing\n",
        "            dense_row[vocabulary[word]] = tf * idf\n",
        "        dense_tfidf_matrix.append(dense_row)\n",
        "\n",
        "    return data, vocabulary, dense_tfidf_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XGEn7nmIHMqV",
        "outputId": "53aba7c5-5151-4b57-8c7f-87bc7065ced5"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "file = \"/content/drive/MyDrive/Colab Notebooks/recipes.csv\"\n",
        "data, vocabulary, dense_tfidf_matrix = preprocess_data(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPV8MJ-gHKQx"
      },
      "outputs": [],
      "source": [
        "# Get recipe recommendations based on cosine similarity\n",
        "def get_recommendations(user_input, vocabulary, dense_tfidf_matrix, data):\n",
        "    user_input = preprocess_user_input(user_input)\n",
        "\n",
        "    # Vectorize user input\n",
        "    user_vector = [0] * len(vocabulary)\n",
        "    inputWords = user_input.split()\n",
        "    for word in inputWords:\n",
        "        if word in vocabulary:\n",
        "            user_vector[vocabulary[word]] += 1\n",
        "\n",
        "    # Compute similarity with each recipe\n",
        "    similarity_scores = []\n",
        "    for recipe_vector in dense_tfidf_matrix:\n",
        "        similarity_scores.append(cosine_similarity(user_vector, recipe_vector))\n",
        "\n",
        "    # Get top 5 recipes\n",
        "    top_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:5]\n",
        "    for index in top_indices:\n",
        "        recipe = data[index]\n",
        "        print(\"Recipe Name:\", recipe['recipe_name'])\n",
        "        print(\"Ingredients:\", recipe['ingredients_text'])\n",
        "        print(\"Ratings:\", recipe['aver_rate'])\n",
        "        print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHF7fa_QHT7v"
      },
      "outputs": [],
      "source": [
        "def kmeans_clustering_optimized(data, k, max_iterations=10, tolerance=1e-4):\n",
        "    # Convert data to a numpy array for vectorized operations\n",
        "    data = np.array(data)\n",
        "    n_samples, n_features = data.shape\n",
        "\n",
        "    # Initialize centroids using K-Means++ for better convergence\n",
        "    centroids = [data[np.random.randint(n_samples)]]\n",
        "    for _ in range(1, k):\n",
        "        distances = np.min([np.linalg.norm(data - centroid, axis=1)**2 for centroid in centroids], axis=0)\n",
        "        probabilities = distances / np.sum(distances)\n",
        "        cumulative_probabilities = np.cumsum(probabilities)\n",
        "        r = np.random.rand()\n",
        "\n",
        "        for j, p in enumerate(cumulative_probabilities):\n",
        "            if r < p:\n",
        "                centroids.append(data[j])\n",
        "                break\n",
        "\n",
        "    centroids = np.array(centroids)\n",
        "\n",
        "    # Iterate to refine centroids\n",
        "    for _ in range(max_iterations):\n",
        "        # Compute cosine similarities in a vectorized manner\n",
        "        dot_products = np.dot(data, centroids.T)\n",
        "        data_norms = np.linalg.norm(data, axis=1, keepdims=True)\n",
        "        centroid_norms = np.linalg.norm(centroids, axis=1, keepdims=True).T\n",
        "        similarities = dot_products / (data_norms @ centroid_norms + 1e-8)\n",
        "\n",
        "        # Assign clusters based on maximum similarity\n",
        "        cluster_labels = np.argmax(similarities, axis=1)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = np.array([\n",
        "            np.mean(data[cluster_labels == c], axis=0) if len(data[cluster_labels == c]) > 0 else centroids[c]\n",
        "            for c in range(k)\n",
        "        ])\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(new_centroids - centroids) < tolerance:\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    # Group data points into clusters\n",
        "    clusters = []\n",
        "    for c in range(k):\n",
        "        cluster_data = data[cluster_labels == c]\n",
        "        clusters.append(cluster_data)\n",
        "    return centroids, clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15byPe_dMuds",
        "outputId": "4a9bf9a4-87c5-4793-8a57-cafdff5bf676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clustering completed with centroids:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "centroids, clusters = kmeans_clustering_optimized(dense_tfidf_matrix, k=3)\n",
        "print(\"Clustering completed with centroids:\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-Cj_x1HqKO",
        "outputId": "f60cf9f1-a39f-41fe-b6d5-8c6ee633cab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recipe Name: Chicken Parmesan Burger\n",
            "Ingredients: \"['ground chicken\n",
            "Ratings: 4\n",
            "--------------------------------------------------\n",
            "Recipe Name: Cheesy Chicken Meatballs\n",
            "Ingredients: \"['ground chicken\n",
            "Ratings: 4.39\n",
            "--------------------------------------------------\n",
            "Recipe Name: Chicken Parmesan Meatball Sliders\n",
            "Ingredients: \"['ground chicken\n",
            "Ratings: 4.43\n",
            "--------------------------------------------------\n",
            "Recipe Name: Chicken Lettuce Wraps\n",
            "Ingredients: \"['ground chicken\n",
            "Ratings: 3.79\n",
            "--------------------------------------------------\n",
            "Recipe Name: Chicken Cordon Bleu Bites\n",
            "Ingredients: \"['ground chicken\n",
            "Ratings: 4.02\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#user_input = input(\"Enter ingredients: \").strip()\n",
        "user_input = \"chicken\"\n",
        "get_recommendations(user_input, vocabulary, dense_tfidf_matrix, data)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}